# -*- coding: utf-8 -*-
import numpy
import torch
import torch.nn as nn
import time
import scipy.spatial

from enum import Enum
from torch.utils.data import Dataset, DataLoader
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeClassifier


class TechniqueType(Enum):
    SCORE_SINGLE_DISTANCE = 1   # distanza tra il gradiente totale e il gradiente di un esempio (ottenuto dal metodo extrapolate_single_gradient)
    SCORE_OTHER_DISTANCE = 2    # distanza tra il gradiente totale e il gradiente totale escludendo un esempio (ottenuto dal metodo extrapolate_other_gradient)
    CLASS_SINGLE_DISTANCE = 3   # come 1, ma con casting del valore della distanza in categorie
    CLASS_OTHER_DISTANCE = 4    # come 2, ma con casting del valore della distanza in categorie


class NeuralNetwork(nn.Module):

  def __init__(self, input_size, output_size):
    super(NeuralNetwork, self).__init__()
    self.linear_1 = torch.nn.Linear(input_size, 100)
    self.linear_2 = torch.nn.Linear(100, output_size)

  def forward(self, x):
    relu = self.linear_1(x).clamp(min=0)
    y_pred = self.linear_2(relu)
    return y_pred

  '''
  Restituisce un vettore di lunghezza 100 o 200 ?   TO CHECK
  '''
  def get_gradient(self):
       return numpy.append(self.linear_2.weight.grad[0].numpy(), self.linear_1.weight.grad[0].numpy())

  def fit(self, dataloader, optimizer, criterion, epochs):
    batch_losses = [] # I konw that I'm not using it now
    for epoch in range(epochs):
      dataiter = iter(dataloader)
      for batch in dataiter:
        optimizer.zero_grad()
        output = self(batch["x"])
        loss = criterion(output, batch["y"])
        batch_losses.append(loss.item())
        loss.backward()
        # aggiorniamo i pesi della rete
        optimizer.step()

class NeuralNetworkFakeHumanDataset(Dataset):
    
  def __init__(self, x, y, total_labeled_samples_size = 0, total_unlabeled_samples_size = 0, input_size = 0, output_size = 0):
    if x is None or y is None:
      self.x = torch.randn(total_labeled_samples_size, input_size)
      self.y = torch.randn(total_labeled_samples_size, output_size)
      # human
      x_unlabeled = torch.randn(total_unlabeled_samples_size, input_size)
      y_unlabeled = torch.randn(total_unlabeled_samples_size, output_size)
      iterator = zip(x_unlabeled, y_unlabeled)
      self.unlabeled_elements = dict(iterator)
    
    else:
        self.x = x
        self.y = y

      
  def get_random_selected_labeled_samples(self, quantity):
    x_popped = []
    y_popped = []
    taken = 0
    for key in self.unlabeled_elements.keys():
      x_popped.append(key)
      y_popped.append(self.unlabeled_elements[key])
      taken += 1
      if taken >= quantity:
        break

    for key in x_popped:
      self.unlabeled_elements.pop(key)

    return x_popped, y_popped

  def get_all_unlabeled_samples(self):
    return list(self.unlabeled_elements.keys())

  def annotate(self, x_to_label):
    x_popped = []
    y_popped = []
    for key in x_to_label:
      x_popped.append(key)
      y_popped.append(self.unlabeled_elements.pop(key))

    return x_popped, y_popped

  def __getitem__(self, index):
    return {"x": self.x[index], "y": self.y[index]}
      
  def __len__(self):
    if type(self.x) is list:
      return len(self.x)

    else:
      return self.x.shape[0]

''' 
Crea il dataset per il selettore
'''
class SamplesDataset():
    
  def __init__(self):
    self.xs = []
    self.ys = []
    
  def add_sample(self, x, y):
    self.xs.append(x)
    self.ys.append(y)

  def get_train_dataset(self):
    return numpy.array(self.xs), numpy.array(self.ys)
      
  def __getitem__(self, index):
    return {"x": self.x[index], "y": self.y[index]}
      
  def __len__(self):
    return self.xs.shape[0]

'''
Assunzione:  3 classi
'''
def value_2_class(value): # todo a review
  print(value)
  if value < 0.33:
    return [1, 0, 0]

  elif 0.33 <= value < 0.66:
    return [0, 1, 0]

  else: # I know that he is not necessary the else but I like it
    return [0, 0, 1]

''' 
Calcola il gradiente relativo ad un esempio in input
'''
def extrapolate_single_gradient(input, output, neural_network, criterion, optimizer):
  optimizer.zero_grad()
  if type(input) is list:
    for i in range(0, len(input)):
      calculated_output = neural_network(input[i])
      loss = criterion(output[i], calculated_output)
      loss.backward()

  else:
    calculated_output = neural_network(input)
    loss = criterion(output, calculated_output)
    loss.backward()

  return neural_network.get_gradient()

'''
Calcola il gradiente relativo ad un batch (annotated_x) w/ one-leave-out (esempio con indice hinge)
'''
def extrapolate_other_gradient(inputs, outputs, hinge, neural_network, criterion, optimizer):
  excluded_x = inputs.pop(hinge)
  excluded_y = outputs.pop(hinge)
  inputs = inputs
  outputs = outputs
  optimizer.zero_grad()
  if type(inputs) is list:
    for i in range(0, len(inputs)):
      calculated_output = neural_network(inputs[i])
      loss = criterion(outputs[i], calculated_output)
      loss.backward()

  else:
    calculated_output = neural_network(inputs)
    loss = criterion(outputs, calculated_output)
    loss.backward()

  inputs.insert(hinge, excluded_x)
  outputs.insert(hinge, excluded_y)
  # taking distance for each sample
  return neural_network.get_gradient()

# defining technique
selector_technique_type = TechniqueType.CLASS_SINGLE_DISTANCE

# defining parameters for technique
quantity_samples_to_select = 10

# defining base parameters (sizes)
total_labeled_samples_size = 500
total_unlabeled_samples_size = 100

# defining base parameters (neural network)
batch_size = 10
input_size = 4
output_size = 3
training_epochs = 7
active_epochs = 5

# defining base parameters (model)
quantity_estimators = 200

# defining model (for selection samples)
if selector_technique_type == TechniqueType.SCORE_SINGLE_DISTANCE or selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE:
  model = GradientBoostingRegressor(loss="lad", n_estimators=quantity_estimators)

else:
  model = DecisionTreeClassifier(max_depth=5)

# deifining Neural Network and realted things
neural_network = NeuralNetwork(input_size, output_size) 
criterion = torch.nn.MSELoss(reduction="sum")    # TO CHECK !
optimizer = torch.optim.SGD(neural_network.parameters(), lr=1e-4)    # da valutare anche Adam

# defining dataloader
neural_network_dataset_fake_human = NeuralNetworkFakeHumanDataset(None, None, total_labeled_samples_size, total_unlabeled_samples_size, input_size, output_size)
dataloader = DataLoader(neural_network_dataset_fake_human, batch_size=batch_size)

# defining datasets to build
dataset = SamplesDataset()

# training the network on the known data (start dataset)
global_start_time = time.time()
print("Training neural network...")
start_time = time.time()
neural_network.fit(dataloader, optimizer, criterion, training_epochs)
end_time = time.time()
print("Neural network first training time: {}".format(end_time - start_time))

# training the network on the unknown datas (active learning dataset)
print("Making Active Learning Training...")
just_annotated_x, just_annotated_y = neural_network_dataset_fake_human.get_random_selected_labeled_samples(quantity_samples_to_select)
print("Selector type: {}".format(selector_technique_type))
for a in range(active_epochs):
  distances = numpy.zeros((len(just_annotated_x), training_epochs))  # matrice di taglia numero di esempi annotati dall'umano  x  no. epochs
  for e in range(training_epochs):
    # building dataset for selector
    print("Training epoch n.{}...".format(e))
    start_time = time.time()
    start_gradients = []
    # iterating over all samples to take the initial gradient
    for index in range(len(just_annotated_x)):
      # taking gradient for each sample and putting it in start_gradients[index]
      start_gradients.append(extrapolate_single_gradient(just_annotated_x[index], just_annotated_y[index], neural_network, criterion, optimizer))   # TO CHECK if it works

    if selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE or selector_technique_type == TechniqueType.CLASS_OTHER_DISTANCE:
      # taking gradient for each just annotated example wrt all other samples (to be optimized)
      for s in range(len(just_annotated_x)):
        current_gradient = extrapolate_other_gradient(just_annotated_x, just_annotated_y, s, neural_network, criterion, optimizer)
        distances[s, e] = abs(1 - scipy.spatial.distance.cosine(start_gradients[s], current_gradient))

    else:
      # taking gradient for each just annotated example wrt single sample
      current_gradient = extrapolate_single_gradient(just_annotated_x, just_annotated_y, neural_network, criterion, optimizer)
      # iterating over all samples to keep distances for each sample
      for s in range(len(just_annotated_x)):
        distances[s, e] = abs(1 - scipy.spatial.distance.cosine(start_gradients[s], current_gradient))

    # training neural network with new samples
    print("Training network with new samples...")
    neural_network.fit(DataLoader(NeuralNetworkFakeHumanDataset(just_annotated_x, just_annotated_y), batch_size=batch_size), optimizer, criterion, 1)
    end_time = time.time()
    print("Training epoch time: {}".format(end_time - start_time))

  for i in range(len(just_annotated_x)):
    average = numpy.average(distances[i])  # mediamo le distanze associate all'i-esimo esempio nelle varie epoch
    # saving distance into dataset
    if selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE or selector_technique_type == TechniqueType.SCORE_SINGLE_DISTANCE:
      dataset.add_sample(just_annotated_x[i].tolist(), average)

    elif selector_technique_type == TechniqueType.CLASS_OTHER_DISTANCE or selector_technique_type == TechniqueType.CLASS_SINGLE_DISTANCE:
      dataset.add_sample(just_annotated_x[i].cpu().detach().numpy().reshape(1, -1), value_2_class(average))

  # training selector
  print("Training selector...")
  start_time = time.time()
  model_x, model_y = dataset.get_train_dataset()
  model_x = numpy.reshape(model_x, (len(model_x), input_size))
  model.fit(model_x, model_y)
  end_time = time.time()
  print("Training selector time: {}".format(end_time - start_time))

  print("Selecting next samples...")
  start_time = time.time()
  # labeling samples
  y = []  # lista per contenere gli score (o corrispondenti categorie) per ciaswcuno degli esempi unlabeled
  x = neural_network_dataset_fake_human.get_all_unlabeled_samples()
  for sample in x:
    y.append(model.predict(sample.cpu().detach().numpy().reshape(1, -1))[0])

  # selecting them
  selected = []
  if selector_technique_type == TechniqueType.SCORE_SINGLE_DISTANCE or selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE:
    ppp = numpy.argsort(y)[-quantity_samples_to_select:][::-1]
    for value in ppp:  # METTERE UN COMMENTO e TO CHECK
      selected.append(x[value])
  
  else:
    selected_samples = 0 
    for i, value in enumerate(y):
      if numpy.argmax(y) == 2:
        selected.append(x[i])
        selected_samples += 1
        if selected_samples >= quantity_samples_to_select:
          break 

  end_time = time.time()
  print("Selection time: {}".format(end_time - start_time))

  # make human label sampling
  print("Labeling samples...")
  start_time = time.time()
  just_annotated_x, just_annotated_y = neural_network_dataset_fake_human.annotate(selected) #TODO fix here
  end_time = time.time()
  print("Labeling time: {}".format(end_time - start_time))

global_end_time = time.time()
print("Active Learning training time: {}".format(global_end_time - global_start_time))
print("Completed!")






























# calculating correlations
   # correlations = []
   # for index in range(len(just_annotated_x)): # I know that I can do this in the prevoius for, but now I want a clearest code rather than optimized
    #  correlations.append(scipy.stats.kendalltau(single_distances[index], other_distances[index]))

# CALCOLARE ANCHE IL GRADIENTE DELLA RETE RISPETTO A x,y meno x[index],y[index]                             
# QUANTO PIU QUESTO GRADIENTE e' VICINO AL VERO GRADIENTE TANTO MENO L'ESEMPIO E' STATO UTILE

# SOMMIAMO LE DISTANZE PER LE VARIE EPOCHE TENENDOCI MAGARI ALTRE STATISTISTICHE
# MAX, MIN, 

# -------------------------------------------------------------------------------------------------------------------

# NELL'ALGORTMO SCRITTO DA FRANCESCO VIENE CALCOLATO LO SCORE PER TUTTI GLI ESEMPI
# IN REALTA SI POTREBBE (E' NECESSARIO PER LA SECONDA DISTANZA) LIMITARE IL CALCOLO DELLO SCORE
# AGLI ESEMPI "NUOVI" (QUELLI APPENA ANNOTATI DALL'ESPERTO OPPURE ALL'INIZIO SI FA UNA SCELTA RANDOM)
# ATTENZIONE LIMITANDO LA TECNICA AD UN INSIEME DI ESEMPI AVREMO UN NUMERO PICCOLO DI
# ESEMPI SU CUI ADDESTRARE IL PREDITTORE

# ALLA FINE DOBBIAMO ASSEGNARE UNO SCORE AGLI ESEMPI
# PRENDO LE SOMME E USO COME SCORE (distanza fra il gradiente di addestramento e):
  # SOLO LA PRIMA DISTANZA (il gradiente rispetto al singolo esempio)
  # SOLO LA SECONDA DISTNAZA (il gradiente rispetto a tutti meno il singolo esempio)
  # LA COMBINAZIONE LINEARE DELLE DUE (LA SECONDA ANDREBBE INVERTITA)

# CORRELAZIONE STATISTICA (PEARSON/KENDALL) FRA PRIMA E SECONDA DISTANZA -  (IMPLICAZIONE?)

# APPRENDERE UN PREDITTORE CHE CI STIMI PER NUOVI ESEMPI: 
# (PROVEREI UN PREDITTORE CHE USA QUALCHE FEATURE INTERMEDIA DELLA RETE E UN'ALTRO CHE USA LE INIZIALE)
  # LO SCORE
  # IL PERCENTILE DELL'ESEMPIO (DIVIDERE IN FASCE - UTILE, MEDIO, INUTILE)

# PROVARE A SELEZIONARE I NUOVI ESEMPI DI CUI RICHIEDERE L'ANNOTAZIONE
  # 1) RISPETTO ALLO SCORE/PERCENTILE
  # 2) RISPETTO ALLA COMBINAZIONE DELLO SCORE/PERCENTILE E DELL'INCERTEZZA

# COMPARAZIONE CON LA SELEZIONE RANDOM
# COMPARAZIONE CON SELEZIONE UNCERTAINTY-BASED