import torch

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeClassifier
from exploiting_gradient import exploit_gradient
from technique_type import TechniqueType
from dataset.selector_dataset import SelectorDataset
from dataset.fake_human.test_fake_human_dataset import TestFakeHumanDataset
from dataset.fake_human.cifar10_fake_human_dataset import Cifar10FakeHumanDataset
from neural_network.test_neural_network import TestNeuralNetwork
from neural_network.convolutional_neural_network import ConvolutionalNeuralNetwork


# defining technique
selector_technique_type = TechniqueType.SCORE_SINGLE_DISTANCE

# defining parameters for technique
quantity_samples_to_select = 10

# defining base parameters (sizes)
total_labeled_samples_size = 500
total_unlabeled_samples_size = 10000

# defining base parameters (neural network)
batch_size = 1
input_size = 4
output_size = 3
metric_size = 500
training_epochs = 1
active_epochs = 2

# defining model (for selection samples)
if selector_technique_type == TechniqueType.SCORE_SINGLE_DISTANCE or selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE:
    model = GradientBoostingRegressor(loss="lad", n_estimators=200)

else:
    model = DecisionTreeClassifier(max_depth=5)

# defining Neural Network and realted things
#neural_network = TestNeuralNetwork(input_size, output_size)
neural_network = ConvolutionalNeuralNetwork()
#neural_network.criterion = torch.nn.MSELoss(reduction="sum")
neural_network.criterion = torch.nn.CrossEntropyLoss()
#neural_network.optimizer = torch.optim.SGD(neural_network.parameters(), lr=1e-4)
neural_network.optimizer = torch.optim.Adam(neural_network.parameters(), lr=1e-4, weight_decay=5e-4)

# defining nn's dataset with simulated annotator
#dataset_fake_human = TestFakeHumanDataset(None, None, total_labeled_samples_size, total_unlabeled_samples_size, input_size, output_size, metric_size, batch_size)
dataset_fake_human = Cifar10FakeHumanDataset(None, None, total_labeled_samples_size, total_unlabeled_samples_size, metric_size, batch_size)

# defining selector's dataset to build
dataset_selector = SelectorDataset()

# loading training
exploit_gradient(selector_technique_type, quantity_samples_to_select, batch_size, input_size, training_epochs, active_epochs, model, neural_network, dataset_fake_human, dataset_selector)
