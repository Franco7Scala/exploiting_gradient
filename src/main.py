import numpy
import torch
import random
import sys

from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier
from exploiting_gradient import exploit_gradient
from technique_type import TechniqueType
from dataset.selector_dataset import SelectorDataset
from dataset.fake_human.cifar10_fake_human_dataset import Cifar10FakeHumanDataset
from neural_network.convolutional_neural_network import ConvolutionalNeuralNetwork
from support import cprint, Color
from vae.vae import VAE

if len(sys.argv) != 6 or sys.argv[1] == "help":
    cprint("Usage:\n\t-parameter 1: quantity_samples_to_select\n\t-parameter 2: training_epochs\n\t-parameter 3: active_epochs\n\t-parameter 4: reproducibility\n\t-parameter 5: use features\n\t-parameter 6: technique \n\t\t1 for SCORE_SINGLE_DISTANCE\n\t\t2 for SCORE_OTHER_DISTANCE\n\t\t3 for CLASS_SINGLE_DISTANCE\n\t\t4 for CLASS_OTHER_DISTANCE\n\t\t5 for RANDOM\n\t\t6 for LEAST CONFIDENCE (UNCERTAINTY SAMPLING)\n", Color.RED)
    sys.exit(0)

# setting device
device = "cuda" if torch.cuda.is_available() else "cpu"
cprint("Running on {}...".format(device), Color.BLUE)

# setting reproducibility
reproducibility = bool(sys.argv[4])
if reproducibility:
    torch.manual_seed(0)
    numpy.random.seed(0)
    random.seed(0)

# defining technique
technique_index = int(sys.argv[6])
if technique_index == 1:
    selector_technique_type = TechniqueType.SCORE_SINGLE_DISTANCE

elif technique_index == 2:
    selector_technique_type = TechniqueType.SCORE_OTHER_DISTANCE

elif technique_index == 3:
    selector_technique_type = TechniqueType.CLASS_SINGLE_DISTANCE

elif technique_index == 4:
    selector_technique_type = TechniqueType.CLASS_OTHER_DISTANCE

elif technique_index == 5:
    selector_technique_type = TechniqueType.RANDOM

elif technique_index == 6:
    selector_technique_type = TechniqueType.LEAST_CONFIDENCE

else:
    cprint("Unknown technique!", Color.RED)
    sys.exit(0)

# defining fixed parameters
training_set_size = 50000
training_epochs = int(sys.argv[2])
active_epochs = int(sys.argv[3])
quantity_samples_to_select = int(sys.argv[1])
use_features = bool(sys.argv[5])

# defining base parameters (neural network)
batch_size = 32
input_size = 32 * 32 * 3
output_size = 10

# iterating in order to do all tests
for i in range(1, 10): 
    # defining current percentages
    percentage_labeled_samples = 1 - i * (0.1)
    percentage_unlabeled_samples = i * (0.1)

    # defining base parameters (sizes)
    total_labeled_samples_size = int(training_set_size * percentage_labeled_samples)
    total_unlabeled_samples_size = int(training_set_size * percentage_unlabeled_samples)

    # defining model (for selection samples)
    if selector_technique_type == TechniqueType.SCORE_SINGLE_DISTANCE or selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE:
        model = GradientBoostingRegressor(loss="lad", n_estimators=200)

    else:
        model = RandomForestClassifier(max_depth=5)

    if use_features:
        #TODO
        features_extractor = VAE()

    else:
        features_extractor = None

    # defining Neural Network and related things
    #neural_network = TestNeuralNetwork(input_size, output_size)
    neural_network = ConvolutionalNeuralNetwork(device)
    #neural_network.criterion = torch.nn.MSELoss(reduction="sum")
    neural_network.criterion = torch.nn.CrossEntropyLoss()
    #neural_network.optimizer = torch.optim.SGD(neural_network.parameters(), lr=1e-4)
    neural_network.optimizer = torch.optim.Adam(neural_network.parameters(), lr=1e-4, weight_decay=5e-4)

    # defining nn's dataset with simulated annotator
    #dataset_fake_human = TestFakeHumanDataset(None, None, total_labeled_samples_size, total_unlabeled_samples_size, input_size, output_size, metric_size, batch_size)
    dataset_fake_human = Cifar10FakeHumanDataset(None, None, total_labeled_samples_size, total_unlabeled_samples_size, batch_size)

    # defining selector's dataset to build
    classed = selector_technique_type == TechniqueType.CLASS_SINGLE_DISTANCE or  selector_technique_type == TechniqueType.CLASS_OTHER_DISTANCE
    dataset_selector = SelectorDataset(input_size, classed)

    # loading training
    start_loss, start_accuracy, end_loss, end_accuracy, elapsed_time = exploit_gradient(selector_technique_type, quantity_samples_to_select, training_epochs, active_epochs, model, neural_network, dataset_fake_human, dataset_selector, features_extractor)
    cprint("Split used: {}% labeled and {}% unlabeled on {} samples".format(percentage_labeled_samples*100, percentage_unlabeled_samples*100, training_set_size), Color.LIGHT_MAGENTA, loggable = True)
    cprint("Used technique: {}".format(selector_technique_type), Color.LIGHT_MAGENTA, loggable = True)
    cprint("Active Learning training time: {}".format(elapsed_time), Color.LIGHT_MAGENTA, loggable = True)
    cprint("Loss after first training: {}".format(start_loss), Color.LIGHT_MAGENTA, loggable = True)
    cprint("Loss after exploiting gradient: {}".format(end_loss), Color.LIGHT_MAGENTA, loggable = True)
    cprint("Accuracy after first training: {}".format(start_accuracy), Color.LIGHT_MAGENTA, loggable = True)
    cprint("Accuracy after exploiting gradient: {}".format(end_accuracy), Color.LIGHT_MAGENTA, loggable = True)
    cprint("Saving neural network...", Color.GREEN)
    neural_network.save("./data/networks/cnn_{}_{}_{}.net".format(selector_technique_type, percentage_labeled_samples, percentage_unlabeled_samples))

cprint("Completed!", Color.YELLOW)
