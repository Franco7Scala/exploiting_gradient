import numpy

from torch.utils.data import Dataset, DataLoader


class AbstractFakeHumanDataset(Dataset):

    def __init__(self, x_labeled, y_labeled, x_unlabeled=[], y_unlabeled=[], x_metric=[], y_metric=[], batch_size=32):
        # base dataset
        self.x = x_labeled
        self.y = y_labeled
        self.batch_size = batch_size
        self.generating_dataloader_for_vae = False
        # human
        if len(x_unlabeled) != 0 and len(y_unlabeled) != 0:
            iterator = zip(x_unlabeled, y_unlabeled)
            self.unlabeled_elements = dict(iterator)

        # for metrics
        self.x_metric = x_metric
        self.y_metric = y_metric

    def __getitem__(self, index):
        if self.generating_dataloader_for_vae:
            return {"x": self.x[index], "y": self.x[index]}

        else:
            return {"x": self.x[index], "y": self.y[index]}

    def __len__(self):
        if type(self.x) is list:
            return len(self.x)

        else:
            return self.x.shape[0]

    def get_dataloader(self):
        return DataLoader(self, batch_size=self.batch_size)

    def get_vae_dataloader(self):
        self.generating_dataloader_for_vae = True
        result = DataLoader(self, batch_size=self.batch_size)
        self.generating_dataloader_for_vae = False
        return result

    def get_metrics_dataloader(self):
        return AbstractFakeHumanDataset(self.x_metric, self.y_metric, [], [], [], [], batch_size=self.batch_size).get_dataloader()

    def get_random_labeled_samples(self, quantity):
        x_popped = []
        y_popped = []
        taken = 0
        for key in self.unlabeled_elements.keys():
            x_popped.append(key)
            y_popped.append(self.unlabeled_elements[key])
            taken += 1
            if taken >= quantity:
                break

        for key in x_popped:
            self.unlabeled_elements.pop(key)

        return x_popped, y_popped

    def get_least_confidence_labeled_samples(self, quantity, model):
        x_popped = []
        y_popped = []
        confidences = []
        for key in self.unlabeled_elements.keys():
            x = key
            y = self.unlabeled_elements[key]
            # calculating least confidence
            out_model = model(x).to("cpu").detach().numpy()[0]
            simple_least_confidence = numpy.nanmax(out_model)
            normalized_least_confidence = (1 - simple_least_confidence) * (len(out_model) / (len(out_model) - 1))
            # appending
            if len(x_popped) >= quantity:
                min_confidence = min(confidences)
                min_confidence_index = confidences.index(min_confidence)
                if normalized_least_confidence > min_confidence:
                    x_popped.pop(min_confidence_index)
                    y_popped.pop(min_confidence_index)
                    confidences.pop(min_confidence_index)

            x_popped.append(x)
            y_popped.append(y)
            confidences.append(normalized_least_confidence)

        for key in x_popped:
            self.unlabeled_elements.pop(key)

        return x_popped, y_popped

    def get_all_unlabeled_samples(self):
        return list(self.unlabeled_elements.keys())

    def annotate(self, x_to_label):
        x_popped = []
        y_popped = []
        for key in x_to_label:
            x_popped.append(key)
            y_popped.append(self.unlabeled_elements.pop(key))

        return x_popped, y_popped

    def add_samples(self, x, y):
        self.x.extend(x)
        self.y.extend(y)

    def convert_sample(self, x, y=None):
        return x, y
