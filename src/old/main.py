# -*- coding: utf-8 -*-
import torch
from scipy import spatial
from src.old.small_neural_network import SmallNeuralNetwork


batch_size, input_size, hidden_size, output_size = 500, 1000, 100, 10
epochs = 10
x = torch.randn(batch_size, input_size)
y = torch.randn(batch_size, output_size)
model = SmallNeuralNetwork(input_size, hidden_size, output_size)
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
start_gradients = []

# iterating over all samples to keep gradients
for index in range(batch_size):
    input = x[index]
    output = y[index]
    output_pred = model(input)
    loss = criterion(output_pred, output)
    optimizer.zero_grad()
    loss.backward()
    # taking gradient for each sample
    start_gradients.append(model.linear_2.weight.grad)

for i in range(epochs):
    distances = []
    # iterating over all samples to train
    for index in range(batch_size):
        input = x[index]
        output = y[index]
        output_pred = model(input)
        loss = criterion(output_pred, output)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # taking distance for each sample
        current_gradient = model.linear_2.weight.grad
        distances.append(spatial.distance.cosine(start_gradients[index][0], current_gradient[0]))









# - prendere gradiente per ogni esempio
# - addestro con tutto il dataset e prendo gradiente
# - fare distranza coseno fra gradiente esempio singolo e batch
# addestrare rete con questa distanza



'''

    if t % 100 == 99:
        print(t, loss.item())
        
        
        
#iterating over all samples
for index in range(batch_size):
    input = x[index]
    output = y[index]
    output_pred = model(input)
    loss = criterion(output_pred, output)
    optimizer.zero_grad()
    loss.backward()
    # taking gradient
    gradients.append(model.linear_2.weight.grad)
    optimizer.step()

'''