{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "exploiting_gradient.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python2",
   "language": "python",
   "display_name": "Python 2"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "MvaJKinp7Vg5",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import scipy.stats\n",
    "import scipy.spatial\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/Library/Python/2.7/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff6139d36ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;34m\"numpy in {}. One method of fixing this is to repeatedly uninstall \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"numpy until none is found, then reinstall this version.\")\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerictypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/Library/Python/2.7/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version."
     ],
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YVxntLAZVXlF",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "'''\n",
    "Assunzione:  3 classi\n",
    "'''\n",
    "def value_2_class(value): # todo a review\n",
    "  if value < 0.33:\n",
    "    return [1, 0, 0]\n",
    "\n",
    "  else if value >= 0.33 and value < 0.66:\n",
    "    return [0, 1, 0]\n",
    "\n",
    "  else: # I know that he is not necessary the else but I like it\n",
    "    return [0, 0, 1]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-pGCN-vgYbge",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "\n",
    "''' \n",
    "Calcola il gradiente relativo ad un esempio in input\n",
    "'''\n",
    "def extrapolate_single_gradient(input, output, neural_network, criterion, optimizer):\n",
    "  output_predicted = neural_network(input)\n",
    "  loss = criterion(output_predicted, output)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  return neural_network.get_gradient()\n",
    "  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vEN4hS8GbrUj",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "'''\n",
    "Calcola il gradiente relativo ad un batch (annotated_x) w/ one-leave-out (esempio con indice s)\n",
    "'''\n",
    "def extrapolate_other_gradient(annotated_x, annotated_y, s, neural_network, criterion, optimizer):\n",
    "  excluded_x = annotated_x.pop(s)\n",
    "  excluded_y = annotated_y.pop(s)\n",
    "  inputs = annotated_x\n",
    "  outputs = annotated_y\n",
    "  outputs_predicted = neural_network(outputs)\n",
    "  loss = criterion(outputs_predicted, outputs)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  annotated_x.insert(s, excluded_x)\n",
    "  annotated_y.insert(s, excluded_y)\n",
    "  # taking distance for each sample \n",
    "  return neural_network.get_gradient()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g-8_-AoaFuV4",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class TechniqueType(Enum):\n",
    "    SCORE_SINGLE_DISTANCE = 1   # distanza tra il gradiente totale e il gradiente di un esempio (ottenuto dal metodo extrapolate_single_gradient)\n",
    "    SCORE_OTHER_DISTANCE = 2    # distanza tra il gradiente totale e il gradiente totale escludendo un esempio (ottenuto dal metodo extrapolate_other_gradient)\n",
    "    CLASS_SINGLE_DISTANCE = 3   # come 1, ma con casting del valore della distanza in categorie\n",
    "    CLASS_OTHER_DISTANCE = 4    # come 2, ma con casting del valore della distanza in categorie\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Exy4DCP3UweE",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.linear_1 = torch.nn.Linear(input_size, 100)\n",
    "    self.linear_2 = torch.nn.Linear(100, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    relu = self.linear_1(x).clamp(min=0)\n",
    "    y_pred = self.linear_2(relu)\n",
    "    return y_pred\n",
    "\n",
    "  '''\n",
    "  Restituisce un vettore di lunghezza 100 o 200 ?   TO CHECK\n",
    "  '''\n",
    "  def get_gradient(self):\n",
    "    return self.linear_2.weight.grad[0] + self.linear_1.weight.grad[0]\n",
    "\n",
    "  def fit(self, dataloader, optimizer, criterion, epochs):\n",
    "    batch_losses = [] # I konw that I'm not using it now\n",
    "    for epoch in range(epochs):\n",
    "      dataiter = iter(dataloader)\n",
    "      for batch in dataiter:\n",
    "        optimizer.zero_grad()\n",
    "        output = self(batch[\"x\"])\n",
    "        loss = criterion(output, batch[\"y\"])\n",
    "        batch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        # aggiorniamo i pesi della rete\n",
    "        optimizer.step()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EjGVmQR95GKM",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class NeuralNetworkFakeHumanDataset(Dataset):\n",
    "    \n",
    "  def __init__(self, x, y, total_labeled_samples_size = 0, total_unlabeled_samples_size = 0, input_size = 0, output_size = 0):\n",
    "    if x is null or y is null:\n",
    "      self.x = torch.randn(total_labeled_samples_size, input_size)\n",
    "      self.y = torch.randn(total_labeled_samples_size, output_size)\n",
    "      # human\n",
    "      x_unlabeled = torch.randn(total_unlabeled_samples_size, input_size)\n",
    "      y_unlabeled = torch.randn(total_unlabeled_samples_size, output_size)\n",
    "      iterator = zip(x_unlabeled, y_unlabeled)\n",
    "      self.unlabeled_elements = dict(iterator)\n",
    "    \n",
    "    else:\n",
    "      self.x = x\n",
    "      self.y = y\n",
    "      \n",
    "  def get_random_selected_labeled_samples(self, quantity):\n",
    "    x_popped = []\n",
    "    y_popped = []\n",
    "    taken = 0\n",
    "    for key in self.unlabeled_elements.keys():\n",
    "      x_popped = key\n",
    "      y_popped = unlabeled_elements.pop(key) \n",
    "      taken += 1\n",
    "      if taken >= quantity:\n",
    "        break\n",
    "\n",
    "    return x_popped, y_popped\n",
    "\n",
    "  def give_label_to_samples(self, x_to_label):\n",
    "    x_popped = []\n",
    "    y_popped = []\n",
    "    for key in x_to_label:\n",
    "      x_popped = key\n",
    "      y_popped = unlabeled_elements.pop(key) \n",
    "    \n",
    "    return x_popped, y_popped\n",
    "\n",
    "  def get_all_unlabeled_samples():\n",
    "    return zip(*self.unlabeled_elements.items())\n",
    "\n",
    "  def __getitem__(self, index):  \n",
    "    return {\"x\": self.x[index], \"y\": self.y[index]}\n",
    "      \n",
    "  def __len__(self):\n",
    "    return self.x.shape[0]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N1I1Yhc8P0DW",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "''' \n",
    "Crea il dataset per il selettore\n",
    "'''\n",
    "class SamplesDataset():\n",
    "    \n",
    "  def __init__(self):\n",
    "    self.xs = []\n",
    "    self.ys = []\n",
    "    \n",
    "  def add_sample(self, x, y):\n",
    "    self.xs.append(x)\n",
    "    self.ys.append(y)\n",
    "\n",
    "  def get_train_dataset(self):\n",
    "    return numpy.array(self.xs), numpy.array(self.ys)\n",
    "      \n",
    "  def __getitem__(self, index):  \n",
    "    return {\"x\": self.x[index], \"y\": self.y[index]}\n",
    "      \n",
    "  def __len__(self):\n",
    "    return self.xs.shape[0]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uOnMTi-j7qDr",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# defining technique\n",
    "selector_technique_type = TechniqueType.SCORE_SINGLE_DISTANCE\n",
    "\n",
    "# defining parameters for technique\n",
    "quantity_samples_to_select = 100\n",
    "\n",
    "# defining base parameters (sizes)\n",
    "total_labeled_samples_size = 500\n",
    "total_unlabeled_samples_size = 10000\n",
    "\n",
    "# defining base parameters (neural network)\n",
    "batch_size = 32\n",
    "input_size = 1000 \n",
    "output_size = 10\n",
    "training_epochs = 100\n",
    "active_epochs = 5\n",
    "\n",
    "# defining base parameters (model)\n",
    "quantity_estimators = 200\n",
    "\n",
    "# defining model (for selection samples)\n",
    "model = GradientBoostingRegressor(loss=\"lad\", n_estimators=quantity_estimators)\n",
    "\n",
    "# deifining Neural Network and realted things\n",
    "neural_network = NeuralNetwork(input_size, output_size) \n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")    # TO CHECK !\n",
    "optimizer = torch.optim.SGD(neural_network.parameters(), lr=1e-4)    # da valutare anche Adam\n",
    "\n",
    "# defining dataloader\n",
    "neural_network_dataset_fake_human = NeuralNetworkFakeHumanDataset(null, null, total_labeled_samples_size, total_unlabeled_samples_size, input_size, output_size)\n",
    "dataloader = DataLoader(neural_network_dataset_fake_human, batch_size=batch_size)\n",
    "\n",
    "# defining datasets to build\n",
    "dataset = SamplesDataset()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkXmCcwoZTE8",
    "outputId": "86fe7d28-d589-4517-caed-fab566740ae7",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# training the network on the known data (start dataset)\n",
    "global_start_time = time.time()\n",
    "print(\"Training neural network...\")\n",
    "start_time = time.time()\n",
    "neural_network.fit(dataloader, optimizer, criterion, training_epochs)\n",
    "end_time = time.time()\n",
    "print(\"Neural network first training time: {}\".format(end_time - start_time))\n",
    "\n",
    "# training the network on the unknown datas (active learning dataset)\n",
    "print(\"Making Active Learning Training...\")\n",
    "just_annotated_x, just_annotated_y = neural_network_dataset_fake_human.get_random_selected_labeled_samples(quantity_samples_to_select)\n",
    "print(\"Selector type: {}\".format(selector_technique_type))\n",
    "for a in range(active_epochs):\n",
    "  distances = numpy.zeros((just_annotated_x.size, training_epochs))  # matrice di taglia numero di esempi annotati dall'umano  x  no. epochs\n",
    "  for e in range(training_epochs):\n",
    "    # building dataset for selector\n",
    "    print(\"Training epoch n.{}...\".format(training_epochs))\n",
    "    start_time = time.time()\n",
    "    start_gradients = numpy.zeros((just_annotated_x.size))\n",
    "    # iterating over all samples to take the initial gradient\n",
    "    for index in range(just_annotated_x.size):\n",
    "      # taking gradient for each sample and putting it in start_gradients[index]\n",
    "      start_gradients[index] = extrapolate_single_gradient(just_annotated_x[index], just_annotated_y[index], neural_network, criterion, optimizer)   # TO CHECK if it works\n",
    "\n",
    "    if selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE or selector_technique_type == TechniqueType.CLASS_OTHER_DISTANCE:\n",
    "      # taking gradient for each just annotated example wrt all other samples (to be optimized)\n",
    "      for s in range(just_annotated_x.size):\n",
    "        current_gradient = extrapolate_other_gradient(just_annotated_x, just_annotated_y, s, neural_network, criterion, optimizer, start_gradients[s])\n",
    "        distances[s, e] = spatial.distance.cosine(start_gradients[s], current_gradient)\n",
    "\n",
    "    else:\n",
    "      # taking gradient for each just annotated example wrt single sample\n",
    "      current_gradient = extrapolate_single_gradient(just_annotated_x, just_annotated_y, neural_network, criterion, optimizer)\n",
    "      # iterating over all samples to keep distances for each sample\n",
    "      for s in range(just_annotated_x.size):\n",
    "        distances[s, e] = spatial.distance.cosine(start_gradients[s], current_gradient)\n",
    "\n",
    "    # training neural network with new samples\n",
    "    print(\"Training network with new samples...\")\n",
    "    neural_network.fit(DataLoader(NeuralNetworkFakeHumanDataset(just_annotated_x, just_annotated_y), batch_size=batch_size), optimizer, criterion, 1)\n",
    "    end_time = time.time()\n",
    "    print(\"Training epoch time: {}\".format(end_time - start_time))\n",
    "\n",
    "\n",
    "  for i in just_annotated_x.size:\n",
    "    sorted = numpy.sort(distances[i])\n",
    "    sum = numpy.sum(sorted)  # sommiamo le distanze associate all'i-esimo esempio nelle varie epoch\n",
    "    # saving distance into dataset\n",
    "    if selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE or selector_technique_type == TechniqueType.SCORE_SINGLE_DISTANCE:\n",
    "      dataset.add_sample(just_annotated_x[i], sum)\n",
    "\n",
    "    else if selector_technique_type == TechniqueType.CLASS_OTHER_DISTANCE or selector_technique_type == TechniqueType.CLASS_SINGLE_DISTANCE:\n",
    "      dataset.add_sample(just_annotated_x[i], value_2_class(sum))\n",
    "\n",
    "\n",
    "\n",
    "  # training selector\n",
    "  print(\"Training selector...\")\n",
    "  start_time = time.time()\n",
    "  model.fit(dataset.get_train_dataset())\n",
    "  end_time = time.time()\n",
    "  print(\"Training selector time: {}\".format(end_time - start_time))\n",
    "\n",
    "  print(\"Selecting next samples...\")\n",
    "  start_time = time.time()\n",
    "  # labeling samples\n",
    "  y = []  # lista per contenere gli score (o corrispondenti categorie) per ciaswcuno degli esempi unlabeled\n",
    "  x, _ = neural_network_dataset_fake_human.get_all_unlabeled_samples()\n",
    "  for sample in x: \n",
    "    y.append(model.predict(sample))\n",
    "\n",
    "\n",
    "  # selecting them\n",
    "  selected = []\n",
    "  if selector_technique_type == TechniqueType.SCORE_SINGLE_DISTANCE or selector_technique_type == TechniqueType.SCORE_OTHER_DISTANCE:\n",
    "    for value in numpy.argsort(y)[-quantity_samples_to_select:][::-1]:  # METTERE UN COMMENTO e TO CHECK\n",
    "      selected.append(y[value])\n",
    "  \n",
    "  else:\n",
    "    selected_samples = 0 \n",
    "    for i, value in enumerate(y):\n",
    "      if numpy.argmax(y) == 2:\n",
    "        selected.append(x[value])\n",
    "        selected_samples += 1\n",
    "        if selected_samples >= quantity_samples_to_select:\n",
    "          break \n",
    "\n",
    "  end_time = time.time()\n",
    "  print(\"Selection time: {}\".format(end_time - start_time))\n",
    "\n",
    "  # make human label sampling\n",
    "  print(\"Labeling samples...\")\n",
    "  start_time = time.time()\n",
    "  just_annotated_x, just_annotated_y = neural_network_dataset_fake_human.give_label_to_samples(selected)\n",
    "  end_time = time.time()\n",
    "  print(\"Labeling time: {}\".format(end_time - start_time))\n",
    "\n",
    "global_end_time = time.time()\n",
    "print(\"Active Learning training time: {}\".format(global_end_time - global_start_time))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UEv2GsbHLd8E",
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "\n",
    "\n",
    "# calculating correlations\n",
    "    correlations = []\n",
    "    for index in range(just_annotated_x.size): # I know that I can do this in the prevoius for, but now I want a clearest code rather than optimized\n",
    "      correlations.append(scipy.stats.kendalltau(single_distances[index], other_distances[index]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CALCOLARE ANCHE IL GRADIENTE DELLA RETE RISPETTO A x,y meno x[index],y[index]                             \n",
    "# QUANTO PIU QUESTO GRADIENTE e' VICINO AL VERO GRADIENTE TANTO MENO L'ESEMPIO E' STATO UTILE\n",
    "\n",
    "# SOMMIAMO LE DISTANZE PER LE VARIE EPOCHE TENENDOCI MAGARI ALTRE STATISTISTICHE\n",
    "# MAX, MIN, \n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# NELL'ALGORTMO SCRITTO DA FRANCESCO VIENE CALCOLATO LO SCORE PER TUTTI GLI ESEMPI\n",
    "# IN REALTA SI POTREBBE (E' NECESSARIO PER LA SECONDA DISTANZA) LIMITARE IL CALCOLO DELLO SCORE\n",
    "# AGLI ESEMPI \"NUOVI\" (QUELLI APPENA ANNOTATI DALL'ESPERTO OPPURE ALL'INIZIO SI FA UNA SCELTA RANDOM)\n",
    "# ATTENZIONE LIMITANDO LA TECNICA AD UN INSIEME DI ESEMPI AVREMO UN NUMERO PICCOLO DI\n",
    "# ESEMPI SU CUI ADDESTRARE IL PREDITTORE\n",
    "\n",
    "# ALLA FINE DOBBIAMO ASSEGNARE UNO SCORE AGLI ESEMPI\n",
    "# PRENDO LE SOMME E USO COME SCORE (distanza fra il gradiente di addestramento e):\n",
    "  # SOLO LA PRIMA DISTANZA (il gradiente rispetto al singolo esempio)\n",
    "  # SOLO LA SECONDA DISTNAZA (il gradiente rispetto a tutti meno il singolo esempio)\n",
    "  # LA COMBINAZIONE LINEARE DELLE DUE (LA SECONDA ANDREBBE INVERTITA)\n",
    "\n",
    "# CORRELAZIONE STATISTICA (PEARSON/KENDALL) FRA PRIMA E SECONDA DISTANZA -  (IMPLICAZIONE?)\n",
    "\n",
    "# APPRENDERE UN PREDITTORE CHE CI STIMI PER NUOVI ESEMPI: \n",
    "# (PROVEREI UN PREDITTORE CHE USA QUALCHE FEATURE INTERMEDIA DELLA RETE E UN'ALTRO CHE USA LE INIZIALE)\n",
    "  # LO SCORE\n",
    "  # IL PERCENTILE DELL'ESEMPIO (DIVIDERE IN FASCE - UTILE, MEDIO, INUTILE)\n",
    "\n",
    "# PROVARE A SELEZIONARE I NUOVI ESEMPI DI CUI RICHIEDERE L'ANNOTAZIONE\n",
    "  # 1) RISPETTO ALLO SCORE/PERCENTILE\n",
    "  # 2) RISPETTO ALLA COMBINAZIONE DELLO SCORE/PERCENTILE E DELL'INCERTEZZA\n",
    "\n",
    "# COMPARAZIONE CON LA SELEZIONE RANDOM\n",
    "# COMPARAZIONE CON SELEZIONE UNCERTAINTY-BASED"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}